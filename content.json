{"pages":[{"title":"404","text":"404","link":"/404.html"}],"posts":[{"title":"MariaDB、JDK及Redis安装指南","text":"MariaDB安装与配置简介 MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。开发这个分支的原因之一是：甲骨文公司收购了MySQL后，有将MySQL闭源的潜在风险，因此社区采用分支的方式来避开这个风险。 安装准备 查看系统版本 12[root@xukun94 ~]# cat /etc/redhat-releaseCentOS Linux release 7.7.1908 (Core) 查看是否已经安装 1234[root@xukun94 ~]# rpm -qa | grep mariadbmariadb-libs-5.5.65-1.el7.x86_64mariadb-server-5.5.65-1.el7.x86_64mariadb-5.5.65-1.el7.x86_64 以上代表安装过，可卸载重装，卸载命令 1[root@xukun94 ~]# yum remove mariadb-server 开始安装通过yum安装即可，yum命令如下 1[root@xukun94 ~]# yum install mariadb-server 启动服务 1[root@xukun94 ~]# systemctl start mariadb 若需要开机自动启动，执行以下命令即可 1[root@xukun94 ~]# systemctl enable mariadb 配置首次配置 进行初始化配置 1[root@xukun94 ~]# mysql_secure_installation 配置过程如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MariaDB to secure it, we'll need the currentpassword for the root user. If you've just installed MariaDB, andyou haven't set the root password yet, the password will be blank,so you should just press enter here.# 输入数据库管理员密码，若第一次进入则直接回车Enter current password for root (enter for none): OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MariaDBroot user without the proper authorisation.# 设置密码Set root password? [Y/n] y# 密码New password: # 确认密码Re-enter new password: Password updated successfully!Reloading privilege tables.. ... Success!By default, a MariaDB installation has an anonymous user, allowing anyoneto log into MariaDB without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.# 是否移除匿名用户Remove anonymous users? [Y/n] y ... Success!Normally, root should only be allowed to connect from 'localhost'. Thisensures that someone cannot guess at the root password from the network.# 是否拒绝远程登录Disallow root login remotely? [Y/n] n ... skipping.By default, MariaDB comes with a database named 'test' that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.# 是否移除test测试数据库Remove test database and access to it? [Y/n] y - Dropping test database... ... Success! - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.# 是否重新加载权限库Reload privilege tables now? [Y/n] y ... Success!Cleaning up...All done! If you've completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB! 测试是否能登录成功 1234567891011[root@xukun94 ~]# mysql -u root -pEnter password: Welcome to the MariaDB monitor. Commands end with ; or \\g.Your MariaDB connection id is 10Server version: 5.5.65-MariaDB MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.MariaDB [(none)]&gt; 配置字符集为UTF-8 编辑/etc/my.cnf 文件 1[root@xukun94 ~]# vi /etc/my.cnf [mysqld]标签下添加如下代码 12345init_connect='SET collation_connection = utf8_unicode_ci'init_connect='SET NAMES utf8'character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake 编辑/etc/my.cnf.d/client.cnf 文件，在[client]标签下添加 1default-character-set=utf8 编辑/etc/my.cnf.d/mysql-clients.cnf文件，在[mysql]标签下添加 1default-character-set=utf8 重启服务后生效 1[root@xukun94 ~]# systemctl restart mariadb 登录MariaDB，查看编码是否生效 123456789101112131415161718192021222324MariaDB [(none)]&gt; show variables like &quot;%character%&quot;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)MariaDB [(none)]&gt; show variables like &quot;%collation%&quot;;+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_unicode_ci || collation_database | utf8_unicode_ci || collation_server | utf8_unicode_ci |+----------------------+-----------------+3 rows in set (0.00 sec) 修改防火墙，开启端口 123456789101112# 查看防火墙3306端口是否开启[root@xukun94 ~]# firewall-cmd --query-port=3306/tcpno# 开启防火墙3306端口[root@xukun94 ~]# firewall-cmd --zone=public --add-port=3306/tcp --permanentsuccess# 重启防火墙[root@xukun94 ~]# firewall-cmd --reloadsuccess# 查看防火墙3306端口是否开启[root@xukun94 ~]# firewall-cmd --query-port=3306/tcpyes 设置MariaDB可远程连接 查看mysql库中user表 12345678910111213141516MariaDB [(none)]&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [mysql]&gt; select host, user from user -&gt; ;+-----------+------+| host | user |+-----------+------+| 127.0.0.1 | root || ::1 | root || localhost | root || xukun94 | root |+-----------+------+4 rows in set (0.00 sec) 将host为当前主机名的记录，host属性修改成’%’ 123MariaDB [mysql]&gt; update user set host='%' where host='xukun94';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0 刷新权限表 12MariaDB [mysql]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 远程连接配置完成，使用连接工具连接即可 JDK1.8安装与配置安装准备 查看系统位数 I386或I686 都是32位 x86_64 是 64位 12[root@xukun94 ~]# uname -aLinux xukun94 3.10.0-229.1.2.el7.x86_64 #1 SMP Fri Mar 27 03:04:26 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux 进入Oracle官网下载JDK1.8，选择对应系统位数的rpm下载 使用xftp6等传输工具将下载好的文件上传到远程服务器中 开始安装使用rpm命令安装 1[root@xukun94 ~]# rpm -ivh jdk-8u261-linux-x64.rpm 配置添加环境变量，编辑source /etc/profile文件 1[root@xukun94 ~]# vi /etc/profile 在此文件最后添加如下代码 12export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH 生效环境变量 1[root@xukun94 ~]# source /etc/profile 查看jdk是否安装成功 1234[root@xukun94 ~]# java -versionjava version &quot;1.8.0_261&quot;Java(TM) SE Runtime Environment (build 1.8.0_261-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode) Redis安装与配置安装准备 使用wget命令下载Redis5.0.9 1[root@xukun94 ~]# wget http://download.redis.io/releases/redis-5.0.9.tar.gzwget http://download.redis.io/releases/redis-5.0.9.tar.gz 解压 1[root@xukun94 ~]# tar xzf redis-5.0.9.tar.gz 开始安装 进入解压后文件夹，执行make编译命令 12[root@xukun94 ~]# cd redis-5.0.9[root@xukun94 redis-5.0.9]# make 若遇到一下错误，说明gcc未安装，使用yum命令安装即可 123456789101112131415MAKE hirediscd hiredis &amp;&amp; make staticmake[3]: Entering directory `/root/redis-5.0.9/deps/hiredis'gcc -std=c99 -pedantic -c -O3 -fPIC -Wall -W -Wstrict-prototypes -Wwrite-strings -g -ggdb net.cmake[3]: gcc: Command not foundmake[3]: *** [net.o] Error 127make[3]: Leaving directory `/root/redis-5.0.9/deps/hiredis'make[2]: *** [hiredis] Error 2make[2]: Leaving directory `/root/redis-5.0.9/deps'make[1]: [persist-settings] Error 2 (ignored) CC adlist.o/bin/sh: cc: command not foundmake[1]: *** [adlist.o] Error 127make[1]: Leaving directory `/root/redis-5.0.9/src'make: *** [all] Error 2 yum安装gcc 1[root@xukun94 redis-5.0.9]# yum install gcc 再次运行make编译命令，可能遇到以下错误 1234567891011121314cd src &amp;&amp; make allmake[1]: Entering directory `/root/redis-5.0.9/src' CC Makefile.depmake[1]: Leaving directory `/root/redis-5.0.9/src'make[1]: Entering directory `/root/redis-5.0.9/src' CC adlist.oIn file included from adlist.c:34:0:zmalloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory #include &lt;jemalloc/jemalloc.h&gt; ^compilation terminated.make[1]: *** [adlist.o] Error 1make[1]: Leaving directory `/root/redis-5.0.9/src'make: *** [all] Error 2 执行以下代码进行编译 1[root@xukun94 redis-5.0.9]# make MALLOC=libc 没有错误后，安装命令 1[root@xukun94 redis-5.0.9]# make install 配置 编辑redis.conf文件 1[root@xukun94 redis-5.0.9]# vi redis.conf 修改以下配置 1234# redis后台运行daemonize yes# 添加ipbind 127.0.0.1 192.168.136.128 允许远程访问redis 123456789101112# 查看防火墙6379端口是否开启[root@xukun94 redis-5.0.9]# firewall-cmd --query-port=6379/tcpno# 开启防火墙6379端口[root@xukun94 redis-5.0.9]# firewall-cmd --zone=public --add-port=6379/tcp --permanentsuccess# 重启防火墙[root@xukun94 redis-5.0.9]# firewall-cmd --reloadsuccess# 再次查看防火墙6379端口是否开启[root@xukun94 redis-5.0.9]# firewall-cmd --query-port=6379/tcpyes 启动redis 1[root@xukun94 redis-5.0.9]# redis-server redis.conf 登录redis，验证是否安装成功 12[root@xukun94 redis-5.0.9]# redis-cli127.0.0.1:6379&gt; exit 关闭服务器 1[root@xukun94 redis-5.0.9]# redis-cli shutdown","link":"/2021-11-21/f01fc1ab108a/"},{"title":"Docker入门","text":"1 Docker简介1.1 什么是虚拟化​ 在计算机中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。 ​ 在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件 对资源充分利用 ​ 虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化(vip)、桌面虚拟化、服务虚拟化、虚拟机等等。 1.2 什么是Docker​ Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 ​ Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 ​ Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。 ​ 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 为什么选择Docker? （1）上手快。 ​ 用户只需要几分钟，就可以把自己的程序“Docker化”。Docker依赖于“写时复制”（copy-on-write）模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改”的境界。 ​ 随后，就可以创建容器来运行应用程序了。大多数Docker容器只需要不到1秒中即可启动。由于去除了管理程序的开销，Docker容器拥有很高的性能，同时同一台宿主机中也可以运行更多的容器，使用户尽可能的充分利用系统资源。 （2）职责的逻辑分类 ​ 使用Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如何管理容器。Docker设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题（测试环境都是正常的，上线后出了问题就归结为肯定是运维的问题）” （3）快速高效的开发生命周期 ​ Docker的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用程序具备可移植性，易于构建，并易于协作。（通俗一点说，Docker就像一个盒子，里面可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件件的取。） （4）鼓励使用面向服务的架构 ​ Docker还鼓励面向服务的体系结构和微服务架构。Docker推荐单个容器只运行一个应用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。（当然，可以在一个容器中运行多个应用程序） 1.3 容器与虚拟机比较​ 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。 与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。 1.4 Docker 组件1.4.1 Docker服务器与客户端​ Docker是一个客户端-服务器（C/S）架构程序。Docker客户端只需要向Docker服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker提供了一个命令行工具Docker以及一整套RESTful API。你可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。 1.4.2 Docker镜像与容器​ 镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。例如： 添加一个文件； 执行一个命令； 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更新。 ​ Docker可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。 所以Docker容器就是： ​ 一个镜像格式； ​ 一些列标准操作； ​ 一个执行环境。 ​ Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。 和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。 Docker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。 1.4.3 Registry（注册中心）​ Docker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营公共的Registry叫做Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像（说明：在Docker Hub下载镜像巨慢，可以自己构建私有的Registry）。https://hub.docker.com/ 2 Docker安装与启动2.1 安装Docker​ Docker官方建议在Ubuntu中安装，因为Docker是基于Ubuntu发布的，而且一般Docker出现的问题Ubuntu是最先更新或者打补丁的。在很多版本的CentOS中是不支持更新最新的一些补丁包的。 ​ 由于我们学习的环境都使用的是CentOS，因此这里我们将Docker安装到CentOS上。注意：这里建议安装在CentOS7.x以上的版本，在CentOS6.x的版本中，安装前需要安装其他很多的环境而且Docker很多补丁不支持更新。 （1）yum 包更新到最新 1sudo yum update （2）安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 （3）设置yum源为阿里云 1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo （4）安装docker 1sudo yum install docker-ce （5）安装后查看docker版本 1docker -v 2.2 设置ustc的镜像​ ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。ustc的docker镜像加速器速度很快。ustc docker mirror的优势之一就是不需要注册，是真正的公共服务。 https://lug.ustc.edu.cn/wiki/mirrors/help/docker 编辑该文件： 1vi /etc/docker/daemon.json 在该文件中输入如下内容： 123{ &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]} 2.3 Docker的启动与停止systemctl命令是系统服务管理器指令 启动docker： 1systemctl start docker 停止docker： 1systemctl stop docker 重启docker： 1systemctl restart docker 查看docker状态： 1systemctl status docker 开机启动： 1systemctl enable docker 查看docker概要信息 1docker info 查看docker帮助文档 1docker --help 3 常用命令3.1 镜像相关命令3.1.1 查看镜像1docker images REPOSITORY：镜像名称 TAG：镜像标签 IMAGE ID：镜像ID CREATED：镜像的创建日期（不是获取该镜像的日期） SIZE：镜像大小 这些镜像都是存储在Docker宿主机的/var/lib/docker目录下 3.1.2 搜索镜像如果你需要从网络中查找需要的镜像，可以通过以下命令搜索 1docker search 镜像名称 NAME：仓库名称 DESCRIPTION：镜像描述 STARS：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否官方 AUTOMATED：自动构建，表示该镜像由Docker Hub自动构建流程创建的 3.1.3 拉取镜像拉取镜像就是从中央仓库中下载镜像到本地 1docker pull 镜像名称 例如，我要下载centos7镜像 1docker pull centos:7 3.1.4 删除镜像按镜像ID删除镜像 1docker rmi 镜像ID 删除所有镜像 1docker rmi `docker images -q` 3.2 容器相关命令3.2.1 查看容器查看正在运行的容器 1docker ps 查看所有容器 1docker ps –a 查看最后一次运行的容器 1docker ps –l 查看停止的容器 1docker ps -f status=exited 3.2.2 创建与启动容器创建容器常用的参数说明： 创建容器命令：docker run -i：表示运行容器 -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 –name :为创建的容器命名。 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 （1）交互式方式创建容器 1docker run -it --name=容器名称 镜像名称:标签 /bin/bash 这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器 1exit （2）守护式方式创建容器： 1docker run -di --name=容器名称 镜像名称:标签 登录守护式容器方式： 1docker exec -it 容器名称 (或者容器ID) /bin/bash 3.2.3 停止与启动容器停止容器： 1docker stop 容器名称（或者容器ID） 启动容器： 1docker start 容器名称（或者容器ID） 3.2.4 文件拷贝如果我们需要将文件拷贝到容器内可以使用cp命令 1docker cp 需要拷贝的文件或目录 容器名称:容器目录 也可以将文件从容器内拷贝出来 1docker cp 容器名称:容器目录 需要拷贝的文件或目录 3.2.5 目录挂载​ 我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。​ 创建容器 添加-v参数 后边为 宿主机目录:容器目录，例如： 1docker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos3 centos:7 ​ 如果你共享的是多级的目录，可能会出现权限不足的提示。 ​ 这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 –privileged=true 来解决挂载的目录没有权限的问题 3.2.6 查看容器IP地址我们可以通过以下命令查看容器运行的各种数据 1docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址 1docker inspect --format='{{.NetworkSettings.IPAddress}}' 容器名称（容器ID） 3.2.7 删除容器删除指定的容器： 1docker rm 容器名称（容器ID） 4 应用部署4.1 MySQL部署（1）拉取mysql镜像 1docker pull centos/mysql-57-centos7 （2）创建容器 1docker run -di --name=tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql -p 代表端口映射，格式为 宿主机映射端口:容器运行端口 -e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码 （3）远程登录mysql 连接宿主机的IP,指定端口为33306 4.2 tomcat部署（1）拉取镜像 1docker pull tomcat:7-jre7 （2）创建容器 创建容器 -p表示地址映射 12docker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7 4.3 Nginx部署（1）拉取镜像 1docker pull nginx （2）创建Nginx容器 1docker run -di --name=mynginx -p 80:80 nginx 4.4 Redis部署（1）拉取镜像 1docker pull redis （2）创建容器 1docker run -di --name=myredis -p 6379:6379 redis 5 迁移与备份5.1 容器保存为镜像我们可以通过以下命令将容器保存为镜像 1docker commit mynginx mynginx_i 5.2 镜像备份我们可以通过以下命令将镜像保存为tar 文件 1docker save -o mynginx.tar mynginx_i 5.3 镜像恢复与迁移首先我们先删除掉mynginx_img镜像 然后执行此命令进行恢复 1docker load -i mynginx.tar -i 输入的文件 执行后再次查看镜像，可以看到镜像已经恢复 6 Dockerfile6.1 什么是DockerfileDockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 1、对于开发人员：可以为开发团队提供一个完全一致的开发环境；2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了；3、对于运维人员：在部署时，可以实现应用的无缝移植。 6.2 常用命令 命令 作用 FROM image_name:tag 定义了使用哪个基础镜像启动构建流程 MAINTAINER user_name 声明镜像的创建者 ENV key value 设置环境变量 (可以写多条) RUN command 是Dockerfile的核心部分(可以写多条) ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压 COPY source_dir/file dest_dir/file 和ADD相似，但是如果有压缩文件并不能解压 WORKDIR path_dir 设置工作目录 6.3 使用脚本创建镜像步骤： （1）创建目录 1mkdir –p /usr/local/dockerjdk8 （2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录 （3）创建文件Dockerfile vi Dockerfile 123456789101112131415#依赖镜像名称和IDFROM centos:7#指定镜像创建者信息MAINTAINER XUKUN#切换工作目录WORKDIR /usrRUN mkdir /usr/local/java#ADD 是相对路径jar,把java添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/#配置java环境变量ENV JAVA_HOME /usr/local/java/jdk1.8.0_171ENV JRE_HOME $JAVA_HOME/jreENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHENV PATH $JAVA_HOME/bin:$PATH （4）执行命令构建镜像 1docker build -t='jdk1.8' . 注意后边的空格和点，不要省略 （5）查看镜像是否建立完成 1docker images 7 Docker私有仓库7.1 私有仓库搭建与配置（1）拉取私有仓库镜像（此步省略） 1docker pull registry （2）启动私有仓库容器 1docker run -di --name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://192.168.184.141:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 （4）修改daemon.json 1vi /etc/docker/daemon.json 添加以下内容，保存退出。 1{&quot;insecure-registries&quot;:[&quot;192.168.184.141:5000&quot;]} 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 1systemctl restart docker 7.2 镜像上传至私有仓库（1）标记此镜像为私有仓库的镜像 1docker tag jdk1.8 192.168.184.141:5000/jdk1.8 （2）再次启动私服容器 1docker start registry （3）上传标记的镜像 1docker push 192.168.184.141:5000/jdk1.8 8 DockerMaven插件8.1 微服务部署有两种方法：（1）手动部署：首先基于源码打包生成jar包（或war包）,将jar包（或war包）上传至虚拟机并拷贝至JDK容器。 （2）通过Maven插件自动部署。 ​ 对于数量众多的微服务，手动部署无疑是非常麻烦的做法，并且容易出错。所以我们这里学习如何自动部署，这也是企业实际开发中经常使用的方法。 8.2 Maven插件自动部署步骤：（1）修改宿主机的docker配置，让其可以远程访问 1vi /lib/systemd/system/docker.service 其中ExecStart=后添加配置 1‐H tcp://0.0.0.0:2375 ‐H unix:///var/run/docker.sock 修改后如下： （2）刷新配置，重启服务 123systemctl daemon‐reloadsystemctl restart dockerdocker start registry （3）在工程pom.xml 增加配置 123456789101112131415161718192021222324252627282930&lt;build&gt; &lt;finalName&gt;app&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring‐boot‐maven‐plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!‐‐ docker的maven插件，官网：https://github.com/spotify/docker‐maven‐plugin ‐‐&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker‐maven‐plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.184.141:5000/${project.artifactId}:${project.version} &lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;‐jar&quot;, &quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory} &lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.184.141:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 以上配置会自动生成Dockerfile 123FROM jdk1.8ADD app.jar /ENTRYPOINT [&quot;java&quot;,&quot;‐jar&quot;,&quot;/app.jar&quot;] （5）在windows的命令提示符下，进入工程tensquare_parent所在的目录 1mvn install 进入tensquare_base 所在的目录，输入以下命令，进行打包和上传镜像 1mvn docker:build ‐DpushImage 执行后，会有如下输出，代码正在上传 （6）进入宿主机 , 查看镜像 1docker images 1234REPOSITORY TAG IMAGE ID CREATED SIZE192.168.184.135:5000/tensquare_base 1.0‐SNAPSHOT 83efa6b4478c 10 minutes ago 687.9 MB192.168.184.135:5000/jdk1.8 latest 507438a0158f 6 hours ago 584 MBjdk1.8 latest 507438a0158f 6 hours ago 584 MB 输出如上内容，表示微服务已经做成镜像浏览器访问 http://192.168.184.141:5000/v2/_catalog ，输出 1{&quot;repositories&quot;:[&quot;tensquare_base&quot;]} （7） 启动容器： 1docker run ‐di ‐‐name=base ‐p 9001:9001 192.168.184.141:5000/tensquare_base:1.0‐SNAPSHOT","link":"/2021-11-21/4ada93cc441a/"},{"title":"Hexo+Icarus+Github搭建个人博客","text":"Hexo+GitHub是一种低成本、快速搭个人博客的方案，本文主要简述我在搭建个人博客过程。 Hexo安装什么是 Hexo？ Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装前提安装 Hexo 相当简单，只需要先安装下列应用程序即可： Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本) Git并配置好Github ssh免密登录 开始安装所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo，在需要安装的目录打开nodejs控制台输入以下命令： 12345npm install -g hexo-clihexo init blogcd blognpm installhexo server 经过这一顿操作，不出意外你的博客就已经在你本地启动起来了，控制台会输出一个网址http://localhost:4000/，在浏览器打开这个网址就能看见你的博客了。 Icarus快速上手Icarus是静态网站生成器Hexo的一款简单，精致，而现代的主题。 它力求设计上的优雅，但也不抛弃使用上的简单明了。 它灵活且多功能的配置系统让资深用户也能极尽细节地装饰他们的站点。 Icarus同时也提供了超多插件与挂件来满足你的多元的站点个性化和优化需求。 除此以外，它的崭新实现使得更好的IDE支持和第三方接入成为可能，并提供了更多未来的优化空间。 从源码安装下载主题源码 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus -b 4.4.0 --depth 1 使用hexo命令修改主题为Icarus 1hexo config theme icarus 最后，使用如下命令来启动Hexo本地测试服务器并开始创作。 12npm installhexo server 打开页面http://localhost:4000/，顺眼许多。 遇到的问题，缺少npm包 根据报错信息，安装所需npm包 1npm install --save bulma-stylus@0.8.0 hexo-renderer-inferno@^0.1.3 hexo-component-inferno@^0.13.0 inferno@^7.3.3 inferno-create-element@^7.3.3 Hexo及主题配置Hexo配置参考：https://hexo.io/zh-cn/docs/configuration.html Icarus主题配置参考：https://ppoffice.github.io/hexo-theme-icarus/Configuration/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/ 部署到Github创建用户名同名的Github仓库，Github 仅能使用一个同名仓库的代码托管一个静态站点。 安装部署插件 1npm install hexo-deployer-git --save 配置_config.yml文件 1234deploy: type: 'git' repo: git@github.com:xukun0904/xukun0904.github.io.git branch: master 生成静态文件，并部署上传到Github 12hexo generatehexo deploy 之后访问你的用户名.github.io可以访问到你的个人博客了，Enjoy It 更多Hexo命令参考：https://hexo.io/zh-cn/docs/commands 跟过主题组件配置参考：https://ppoffice.github.io/hexo-theme-icarus/tags/Icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97/","link":"/2021-11-21/0077e93fe6de/"},{"title":"Mybatis-Plus分页查询数据重复","text":"问题描述​ 使用MyBatis-Plus 3.1.0，在做一个千万级数据从MySQL导入ElasticSearch的测试脚本中，由于数据量过大，不能一次获取，所以需要分页，循环递增页码来获取数据中发现每次分页查询结果都一样。 问题原因​ MyBatis 判断是否需要使用一级缓存的代码早于 MyBatis-Plus 分页插件拦截的代码执行，导致获取的结果一直都是一级缓存中的数据。 解决方案​ 在对应XML的SELECT标签中，加入flushCache=”true”属性，调用此查询方法会自动清除一级缓存和二级缓存。","link":"/2021-11-21/3c18081440f0/"},{"title":"Shiro修改角色权限没有生效","text":"问题描述​ 使用Shiro中shiro:hasPermission标签，修改角色权限后相同角色下各个用户权限不一致问题。在角色管理中修改当前登陆人的角色权限，当前用户的权限更新了，但是这个角色下其它用户的权限还是修改前的权限。 问题原因​ shiro:hasPermission进行比较权限时，先通过CacheManager获取了Redis中的当前用户的权限缓存。 ​ 而更新角色权限后，调用Realm中的内置的clearCache方法清除权限缓存，只清除了当前用户的权限缓存。 解决方案​ 需要清除该角色下所有用户的权限缓存，通过RedisSessionDAO获取所有用户信息，然后循环清除用户权限缓存。若需要清除指定用户权限缓存，可将SimplePrincipalCollection对象强转成用户对象，再根据传入用户主键集合判断是否需要清除权限缓存。","link":"/2021-11-21/e66b8f2cdbc7/"},{"title":"kubeadm安装kubernetes集群","text":"1. 安装说明虽然K8s 1.20版本宣布将在1.23版本之后将不再维护dockershim，意味着K8s将不直接支持Docker，不过大家不必过于担心。一是在1.23版本之前我们仍然可以使用Docker，二是dockershim肯定会有人接盘，我们同样可以使用Docker，三是Docker制作的镜像仍然可以在其他Runtime环境中使用，所以大家不必过于恐慌。 本次安装采用的是Kubeadm安装工具，安装版本是K8s 1.22+，采用的系统为CentOS 7.9，其中Master节点3台，Node节点2台，高可用工具采用HAProxy + KeepAlived。 2. 节点规划 主机名 IP地址 角色 配置 k8s-master01 ~ 03 192.168.1.201 ~ 203 Master/Worker节点 2C2G 40G k8s-node01 ~ 02 192.168.1.204 ~ 205 Worker节点 2C2G 40G k8s-master-lb 192.168.1.236 VIP VIP不占用机器 信息 备注 系统版本 CentOS 7.9 Docker版本 19.03.x K8s版本 1.20.x Pod网段 172.168.0.0/16 Service网段 10.96.0.0/12 3. 基本配置静态IP配置，hostname设置 123456789101112vi /etc/sysconfig/network-scripts/ifcfg-ens33BOOTPROTO=&quot;static&quot;IPADDR=192.168.1.201GATEWAY=192.168.1.1DNS1=192.168.1.1service network restarthostname k8s-master01vi /etc/hostnamek8s-master01 所有节点配置hosts 1234567[root@k8s-master01 ~]# cat /etc/hosts192.168.1.201 k8s-master01192.168.1.202 k8s-master02192.168.1.203 k8s-master03192.168.1.236 k8s-master-lb # 如果不是高可用集群，该IP为Master01的IP192.168.1.204 k8s-node01192.168.1.205 k8s-node02 yum源配置 12345678910111213curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repoyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repocat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo 必备工具安装 1yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y 所有节点关闭防火墙、selinux、dnsmasq、swap。服务器配置如下： 1234567systemctl disable --now firewalld systemctl disable --now dnsmasqsystemctl disable --now NetworkManagersetenforce 0sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinuxsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config 关闭swap分区 12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 安装ntpdate 12rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpmyum install ntpdate -y 所有节点同步时间。时间同步配置如下： 123ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeecho 'Asia/Shanghai' &gt;/etc/timezonentpdate time2.aliyun.com 加入到crontab 12crontab -e*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com 所有节点配置limit： 12345678910ulimit -SHn 65535vim /etc/security/limits.conf# 末尾添加如下内容* soft nofile 655360* hard nofile 131072* soft nproc 655350* hard nproc 655350* soft memlock unlimited* hard memlock unlimited Master01节点免密钥登录其他节点： 123ssh-keygen -t rsafor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done 下载安装所有的源码文件： 12cd /root/git clone https://github.com/xukun0904/k8s-ha-install.git 所有节点升级系统并重启： 1yum update -y &amp;&amp; reboot 4. 内核配置所有节点内核升级 123456wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-5.14.3-1.el7.elrepo.x86_64.rpmwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-5.14.3-1.el7.elrepo.x86_64.rpmyum -y install kernel-ml-5.14.3-1.el7.elrepo.x86_64.rpm kernel-ml-devel-5.14.3-1.el7.elrepo.x86_64.rpmcat /boot/grub2/grub.cfg |grep menuentrygrub2-set-default &quot;CentOS Linux (5.14.3-1.el7.elrepo.x86_64) 7 (Core)&quot;grub2-editenv list 所有节点安装ipvsadm： 1yum install ipvsadm ipset sysstat conntrack libseccomp -y 所有节点配置ipvs模块，4.19以下内核nf_conntrack_ipv4 4.19以上内核为nf_conntrack 123456789101112131415161718192021222324vim /etc/modules-load.d/ipvs.conf # 加入以下内容ip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipip 加载内核配置 1systemctl enable --now systemd-modules-load.service 开启一些k8s集群中必须的内核参数，所有节点配置k8s内核 1234567891011121314151617181920212223242526272829cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemrebootlsmod|grep -e ip_vs -e nf_conntrack 5. 基本组件安装所有节点安装Docker-ce 19.03 1yum install docker-ce-19.03.* -y 所有节点设置开机自启动Docker 1systemctl daemon-reload &amp;&amp; systemctl enable --now docker 安装k8s组件 1yum list kubeadm.x86_64 --showduplicates | sort -r 所有节点安装最新版本kubeadm 1yum install kubeadm -y 默认配置的pause镜像使用gcr.io仓库，国内可能无法访问，所以这里配置Kubelet使用阿里云的pause镜像： 123456789101112cat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS=&quot;--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2&quot;EOF# 新版kubelet建议使用systemd，所以可以把cgroupdriver改为systemdcat &gt; /etc/docker/daemon.json &lt;&lt;EOF{ &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]}EOFsystemctl restart docker 设置Kubelet开机自启动 12systemctl daemon-reloadsystemctl enable --now kubelet 6. 高可用组件安装注意：如果不是高可用集群或者在云上安装，haproxy和keepalived无需安装所有Master节点通过yum安装HAProxy和KeepAlived： 1yum install keepalived haproxy -y 所有Master节点配置HAProxy（详细配置参考HAProxy文档，所有Master节点的HAProxy配置相同）： 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master01 etc]# mkdir /etc/haproxy[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30sdefaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15sfrontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitorfrontend k8s-master bind 0.0.0.0:16443 bind 127.0.0.1:16443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-masterbackend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.1.201:6443 check server k8s-master02 192.168.1.202:6443 check server k8s-master03 192.168.1.203:6443 check 所有Master节点配置KeepAlived，配置不一样，注意区分注意每个节点的IP和网卡（interface参数）Master01节点的配置： 12345678910111213141516171819202122232425262728293031323334[root@k8s-master01 etc]# mkdir /etc/keepalived[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf ! Configuration File for keepalivedglobal_defs { router_id LVS_DEVELscript_user root enable_script_security}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1}vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.1.201 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver }} Master02节点的配置： 12345678910111213141516171819202122232425262728293031! Configuration File for keepalivedglobal_defs { router_id LVS_DEVELscript_user root enable_script_security}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1}vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.1.202 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver }} Master03节点的配置： 12345678910111213141516171819202122232425262728293031! Configuration File for keepalivedglobal_defs { router_id LVS_DEVELscript_user root enable_script_security}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1}vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.1.203 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver }} 为所有master节点配置KeepAlived健康检查文件： 1234567891011121314151617181920212223242526272829303132333435[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh #!/bin/basherr=0for k in $(seq 1 3)do check_code=$(pgrep haproxy) if [[ $check_code == &quot;&quot; ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fidoneif [[ $err != &quot;0&quot; ]]; then echo &quot;systemctl stop keepalived&quot; /usr/bin/systemctl stop keepalived exit 1else exit 0fiscp /etc/keepalived/check_apiserver.sh k8s-master02:/etc/keepalived/scp /etc/keepalived/check_apiserver.sh k8s-master03:/etc/keepalived/chmod +x /etc/keepalived/check_apiserver.sh#启动haproxy和keepalived[root@k8s-master01 keepalived]# systemctl daemon-reload[root@k8s-master01 keepalived]# systemctl enable --now haproxy[root@k8s-master01 keepalived]# systemctl enable --now keepalived 测试VIP 123456[root@k8s-master01 ~]# ping 192.168.1.236 -c 4PING 192.168.1.236 (192.168.1.236) 56(84) bytes of data.64 bytes from 192.168.1.236: icmp_seq=1 ttl=64 time=0.464 ms64 bytes from 192.168.1.236: icmp_seq=2 ttl=64 time=0.063 ms64 bytes from 192.168.1.236: icmp_seq=3 ttl=64 time=0.062 ms64 bytes from 192.168.1.236: icmp_seq=4 ttl=64 time=0.063 ms 7. 集群初始化Master01节点创建new.yaml配置文件如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344vim /root/new.yamlapiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: 7t2weq.bjbawausm0jaxury ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.1.201 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master01 taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: certSANs: - 192.168.1.236 timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: 192.168.1.236:16443controllerManager: {}dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: v1.20.0networking: dnsDomain: cluster.local podSubnet: 172.168.0.0/16 serviceSubnet: 10.96.0.0/12scheduler: {} 注意：如果不是高可用集群，192.168.1.236:16443改为master01的地址，16443改为apiserver的端口，默认是6443，注意更改v1.20.0为自己服务器kubeadm的版本：kubeadm version将new.yaml文件复制到其他master节点，之后所有Master节点提前下载镜像，可以节省初始化时间： 123456789scp /root/new.yaml k8s-master02:/root/scp /root/new.yaml k8s-master03:/root/docker pull coredns/coredns:1.8.4docker tag coredns/coredns:1.8.4 registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.4docker rmi coredns/coredns:1.8.4kubeadm config images pull --config /root/new.yaml 所有节点设置开机自启动kubelet（如果启动失败无需管理，初始化成功以后即可启动） 1systemctl enable --now kubelet Master01节点初始化，初始化以后会在/etc/kubernetes目录下生成对应的证书和配置文件，之后其他Master节点加入Master01即可： 1kubeadm init --config /root/new.yaml --upload-certs 初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值）： 123456789101112131415161718192021222324252627282930Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.1.236:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5d5110b03c314d9dda442d6ff25ec251249edf017c5d2ef8f27831bde7a24933 \\ --control-plane --certificate-key 9f98627d614bef2e9d51e2f064ef388a7b764aa7dd25a1dcccae86234e2c8cbfPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.1.236:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5d5110b03c314d9dda442d6ff25ec251249edf017c5d2ef8f27831bde7a24933 Master01节点配置环境变量，用于访问Kubernetes集群： 1234cat &lt;&lt;EOF &gt;&gt; /root/.bashrcexport KUBECONFIG=/etc/kubernetes/admin.confEOFsource /root/.bashrc 查看节点状态： 123 [root@k8s-master01 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady control-plane,master 74s v1.20.0 采用初始化安装方式，所有的系统组件均以容器的方式运行并且在kube-system命名空间内，此时可以查看Pod状态： 123456789[root@k8s-master01 ~]# kubectl get pods -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODEcoredns-777d78ff6f-kstsz 0/1 Pending 0 14m &lt;none&gt; &lt;none&gt;coredns-777d78ff6f-rlfr5 0/1 Pending 0 14m &lt;none&gt; &lt;none&gt;etcd-k8s-master01 1/1 Running 0 14m 192.168.1.201 k8s-master01kube-apiserver-k8s-master01 1/1 Running 0 13m 192.168.1.201 k8s-master01kube-controller-manager-k8s-master01 1/1 Running 0 13m 192.168.1.201 k8s-master01kube-proxy-8d4qc 1/1 Running 0 14m 192.168.1.201 k8s-master01kube-scheduler-k8s-master01 1/1 Running 0 13m 192.168.1.201 k8s-master01 8. 高可用Master初始化其他master加入集群 123456789kubeadm join 192.168.1.236:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5d5110b03c314d9dda442d6ff25ec251249edf017c5d2ef8f27831bde7a24933 \\ --control-plane --certificate-key 9f98627d614bef2e9d51e2f064ef388a7b764aa7dd25a1dcccae86234e2c8cbf#如果初始化失败，重置后再次初始化，命令如下：kubeadm reset -fipvsadm --clearrm -rf ~/.kube Token过期处理方式 token会再两个小时后失效，生成新的token 1kubeadm token create --print-join-command Master需要生成–certificate-key 123kubeadm init phase upload-certs --upload-certskubectl get secret -n kube-systemkubectl get secret -n kube-system bootstrap-token-7t2weq -oyaml 使用新生成的token加入集群 123kubeadm join 192.168.1.236:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5d5110b03c314d9dda442d6ff25ec251249edf017c5d2ef8f27831bde7a24933 \\ --control-plane --certificate-key 9f98627d614bef2e9d51e2f064ef388a7b764aa7dd25a1dcccae86234e2c8cbf 9. 添加Node节点12kubeadm join 192.168.1.236:16443 --token 7t2weq.bjbawausm0jaxury \\ --discovery-token-ca-cert-hash sha256:5d5110b03c314d9dda442d6ff25ec251249edf017c5d2ef8f27831bde7a24933 查看集群状态： 1234567[root@k8s-master01]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady control-plane,master 8m53s v1.20.0k8s-master02 NotReady control-plane,master 2m25s v1.20.0k8s-master03 NotReady control-plane,master 31s v1.20.0k8s-node01 NotReady &lt;none&gt; 32s v1.20.0k8s-node02 NotReady &lt;none&gt; 88s v1.20.0 10. Calico安装以下步骤只在master01执行 1cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.21.x &amp;&amp; cd calico/ 修改calico-etcd.yaml的以下位置 1234567891011121314sed -i 's#etcd_endpoints: &quot;http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;#etcd_endpoints: &quot;https://192.168.1.201:2379,https://192.168.1.202:2379,https://192.168.1.203:2379&quot;#g' calico-etcd.yamlETCD_CA=`cat /etc/kubernetes/pki/etcd/ca.crt | base64 | tr -d '\\n'`ETCD_CERT=`cat /etc/kubernetes/pki/etcd/server.crt | base64 | tr -d '\\n'`ETCD_KEY=`cat /etc/kubernetes/pki/etcd/server.key | base64 | tr -d '\\n'`sed -i &quot;s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcd-cert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g&quot; calico-etcd.yamlsed -i 's#etcd_ca: &quot;&quot;#etcd_ca: &quot;/calico-secrets/etcd-ca&quot;#g; s#etcd_cert: &quot;&quot;#etcd_cert: &quot;/calico-secrets/etcd-cert&quot;#g; s#etcd_key: &quot;&quot; #etcd_key: &quot;/calico-secrets/etcd-key&quot; #g' calico-etcd.yamlPOD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '{print $NF}'`sed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@# value: &quot;192.168.0.0/16&quot;@ value: '&quot;${POD_SUBNET}&quot;'@g' calico-etcd.yaml 创建calico 12345kubectl apply -f calico-etcd.yamlkubectl get nodekubectl get pods -n kube-system -o wide 11. Metrics Server部署在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。将Master01节点的front-proxy-ca.crt复制到所有Node节点 12scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crtscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt 安装metrics server 123456789101112cd /root/k8s-ha-install/metrics-server-0.4.x-kubeadm/[root@k8s-master01 metrics-server-0.4.x-kubeadm]# kubectl create -f comp.yaml serviceaccount/metrics-server createdclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrole.rbac.authorization.k8s.io/system:metrics-server createdrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader createdclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator createdclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server createdservice/metrics-server createddeployment.apps/metrics-server createdapiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created 等待kube-system命令空间下的Pod全部启动后，查看状态 1234567891011121314151617181920212223242526272829303132333435363738kubectl get pods -n kube-system -o wide[root@k8s-master01 metrics-server-0.4.x-kubeadm]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 109m 2% 1296Mi 33% k8s-master02 99m 2% 1124Mi 29% k8s-master03 104m 2% 1082Mi 28% k8s-node01 55m 1% 761Mi 19% k8s-node02 53m 1% 663Mi 17%kubectl top po -n kube-systemNAME CPU(cores) MEMORY(bytes) calico-kube-controllers-cdd5755b9-78fsd 3m 15Mi calico-node-8m2wm 35m 55Mi calico-node-9cb4j 39m 75Mi calico-node-bljgv 43m 77Mi calico-node-chqv8 39m 69Mi calico-node-t97kd 39m 80Mi coredns-7d89d9b6b8-7zvlw 2m 10Mi coredns-7d89d9b6b8-zmth2 2m 11Mi etcd-k8s-master01 54m 90Mi etcd-k8s-master02 44m 85Mi etcd-k8s-master03 39m 84Mi kube-apiserver-k8s-master01 61m 311Mi kube-apiserver-k8s-master02 53m 296Mi kube-apiserver-k8s-master03 51m 283Mi kube-controller-manager-k8s-master01 20m 53Mi kube-controller-manager-k8s-master02 2m 27Mi kube-controller-manager-k8s-master03 2m 24Mi kube-proxy-fln4s 1m 15Mi kube-proxy-fnbns 1m 12Mi kube-proxy-hngtq 1m 17Mi kube-proxy-prk44 1m 17Mi kube-proxy-tn8bt 1m 17Mi kube-scheduler-k8s-master01 5m 22Mi kube-scheduler-k8s-master02 3m 20Mi kube-scheduler-k8s-master03 4m 19Mi metrics-server-d6c46b546-5db6j 4m 26Mi 12. Dashboard部署12345678910111213141516171819cd /root/k8s-ha-install/dashboard/[root@k8s-master01 dashboard]# kubectl create -f .serviceaccount/admin-user createdclusterrolebinding.rbac.authorization.k8s.io/admin-user creatednamespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问Dashboard的问题，参考图： 1--test-type --ignore-certificate-errors 更改dashboard的svc为NodePort： 123kubectl edit svc kubernetes-dashboard -n kubernetes-dashboardkubectl get svc kubernetes-dashboard -n kubernetes-dashboard 将ClusterIP更改为NodePort（如果已经为NodePort忽略此步骤）：查看端口号：根据自己的实例端口号，通过任意安装了kube-proxy的宿主机或者VIP的IP+端口即可访问到dashboard：访问Dashboard：https://192.168.1.236:18282（请更改18282为自己的端口），选择登录方式为令牌（即token方式）查看token值： 1234567891011121314[root@k8s-master01 1.1.1]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')Name: admin-user-token-7rp7rNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 0839a328-b835-4c18-afff-651bf6f18c3fType: kubernetes.io/service-account-tokenData====ca.crt: 1099 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6Ilhma0NUMlRvVWM4S083OGMtd0U0T29DZXpwb2lzODZaa1hfTzdtZXU3LXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTdycDdyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwODM5YTMyOC1iODM1LTRjMTgtYWZmZi02NTFiZjZmMThjM2YiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.l3GdVKFrfZVrELYKtyR9h7bO-HWimYE8O9ZB2XX14GWifsfqvR4_nEHNLph4lZGoLJaNqHX1FR1An7PxDg-miRWIbl5Ms224D0ak4BtpyQUIWcdixNB5xkKJuLOypOiSa1beMYIpcpxQEj_zf3VUTmUFu7bb1EWnwkYXN9leuVoUREEq-Slw--KLA7l6V-3b8H4_mWU5LYaLYxq3G7Svvhn_N7lsn78J0oA4B_ViIjFlGB9TjxcnlTtI8CGPlSb54sJAPDWc804sEp2rit-OldKaSsz_mih7r1wAu8z25zCK6T3Zvj-Eztx8SJiMaGv8ZsIXzbIJrv7mxv17_3aKNA 将token值输入到令牌后，单击登录即可访问Dashboard 一些必须的配置更改 将Kube-proxy改为ipvs模式，因为在初始化集群的时候注释了ipvs配置，所以需要自行修改一下： 在master01节点执行 1kubectl edit cm kube-proxy -n kube-system 修改为 ipvs 1mode: “ipvs” 更新 Kube-Proxy 的 Pod： 1kubectl patch daemonset kube-proxy -p &quot;{\\&quot;spec\\&quot;:{\\&quot;template\\&quot;:{\\&quot;metadata\\&quot;:{\\&quot;annotations\\&quot;:{\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;}}}}}&quot; -n kube-system 在 master03 验证 Kube-Proxy 模式，接着可以在所有服务器验证一下 1curl 127.0.0.1:10249/proxyMode 注意事项kubeadm安装的集群，证书有效期默认是一年。master节点的kube-apiserver、kube-scheduler、kube-controller-manager、etcd都是以容器运行的。可以通过kubectl get po -n kube-system查看。 启动和二进制的区别：kubelet的配置文件在/etc/sysconfig/kubelet和/var/lib/kubelet/config.yaml，修改后需要重启kubelet进程 其他组件的配置文件在/etc/kubernetes/manifests目录下，比如kube-apiserver.yaml，该yaml文件更改后，kubelet会自动刷新配置，也就是会重启pod。不能再次创建该文件 kube-proxy的配置在kube-system命名空间下的configmap中，可以通过 1kubectl edit cm kube-proxy -n kube-system Kubeadm安装后，master节点默认不允许部署pod，会占用资源，在学习过程中可以通过以下方式打开： 查看Taints： 1kubectl describe node -l node-role.kubernetes.io/master= | grep Taints 可以看到三个污点 123Taints: node-role.kubernetes.io/master:NoScheduleTaints: node-role.kubernetes.io/master:NoScheduleTaints: node-role.kubernetes.io/master:NoSchedule 删除Taint： 1kubectl taint node -l node-role.kubernetes.io/master node-role.kubernetes.io/master:NoSchedule- 再次查看： 1234kubectl describe node -l node-role.kubernetes.io/master= | grep TaintsTaints: &lt;none&gt;Taints: &lt;none&gt;Taints: &lt;none&gt; 13.集群验证查看kubernetes服务 12345678910kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 63mkubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 63mmetrics-server ClusterIP 10.101.237.26 &lt;none&gt; 443/TCP 22m 所有节点可以连接kubernetes和kube-dns服务 123456789telnet 10.96.0.1 443Trying 10.96.0.1...Connected to 10.96.0.1.Escape character is '^]'.telnet 10.96.0.10 53Trying 10.96.0.10...Connected to 10.96.0.10.Escape character is '^]'. 所有节点可以ping通pod的ip地址 12345678910111213141516171819202122232425262728293031323334353637kubectl get po --all-namespaces -owideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system calico-kube-controllers-cdd5755b9-78fsd 1/1 Running 0 41m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system calico-node-8m2wm 1/1 Running 0 41m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system calico-node-9cb4j 1/1 Running 0 41m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system calico-node-bljgv 1/1 Running 0 41m 192.168.1.204 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system calico-node-chqv8 1/1 Running 0 41m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system calico-node-t97kd 1/1 Running 0 41m 192.168.1.205 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system coredns-7d89d9b6b8-7zvlw 1/1 Running 0 65m 172.168.32.129 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system coredns-7d89d9b6b8-zmth2 1/1 Running 0 65m 172.168.32.130 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master01 1/1 Running 0 65m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master02 1/1 Running 0 63m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master03 1/1 Running 0 62m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master01 1/1 Running 0 65m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master02 1/1 Running 0 63m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master03 1/1 Running 0 62m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master01 1/1 Running 1 (63m ago) 65m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master02 1/1 Running 0 63m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master03 1/1 Running 0 62m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-fln4s 1/1 Running 0 63m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-fnbns 1/1 Running 0 65m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-hngtq 1/1 Running 0 62m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-prk44 1/1 Running 1 54m 192.168.1.204 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-tn8bt 1/1 Running 1 54m 192.168.1.205 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master01 1/1 Running 1 (63m ago) 65m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master02 1/1 Running 0 63m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master03 1/1 Running 0 62m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system metrics-server-d6c46b546-5db6j 1/1 Running 0 24m 172.168.58.193 k8s-node02 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-86bb69c5f6-ngsvt 1/1 Running 0 23m 172.168.58.194 k8s-node02 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-6576c84894-tmpx9 1/1 Running 0 23m 172.168.85.193 k8s-node01 &lt;none&gt; &lt;none&gt;ping 172.168.58.193 -c 4PING 172.168.58.193 (172.168.58.193) 56(84) bytes of data.64 bytes from 172.168.58.193: icmp_seq=1 ttl=63 time=0.456 ms64 bytes from 172.168.58.193: icmp_seq=2 ttl=63 time=0.356 ms64 bytes from 172.168.58.193: icmp_seq=3 ttl=63 time=0.337 ms64 bytes from 172.168.58.193: icmp_seq=4 ttl=63 time=0.552 ms 非当前机器的pod之间的ip互通 123kubectl exec -it calico-node-bljgv -n kube-system -- shping 172.168.32.130 -c 4 NodePort局域网内可以正常访问","link":"/2021-12-05/88b8770560e1/"},{"title":"二进制安装kubernetes集群","text":"1. 安装说明虽然K8s 1.20版本宣布将在1.23版本之后将不再维护dockershim，意味着K8s将不直接支持Docker，不过大家不必过于担心。一是在1.23版本之前我们仍然可以使用Docker，二是dockershim肯定会有人接盘，我们同样可以使用Docker，三是Docker制作的镜像仍然可以在其他Runtime环境中使用，所以大家不必过于恐慌。 本次安装采用的是二进制安装方式，安装版本是K8s 1.22+，采用的系统为CentOS 7.9，其中Master节点3台，Node节点2台，高可用工具采用HAProxy + KeepAlived。 2. 节点规划 主机名 IP地址 角色 配置 k8s-master01 ~ 03 192.168.1.201 ~ 203 Master/Worker节点 2C2G 40G k8s-node01 ~ 02 192.168.1.204 ~ 205 Worker节点 2C2G 40G k8s-master-lb 192.168.1.236 VIP VIP不占用机器 信息 备注 系统版本 CentOS 7.9 Docker版本 19.03.x K8s版本 1.20.x Pod网段 172.168.0.0/16 Service网段 10.96.0.0/12 3. 基本配置静态IP配置，hostname设置 123456789101112vi /etc/sysconfig/network-scripts/ifcfg-ens33BOOTPROTO=&quot;static&quot;IPADDR=192.168.1.201GATEWAY=192.168.1.1DNS1=192.168.1.1service network restarthostname k8s-master01vi /etc/hostnamek8s-master01 所有节点配置hosts 1234567[root@k8s-master01 ~]# cat /etc/hosts192.168.1.201 k8s-master01192.168.1.202 k8s-master02192.168.1.203 k8s-master03192.168.1.236 k8s-master-lb # 如果不是高可用集群，该IP为Master01的IP192.168.1.204 k8s-node01192.168.1.205 k8s-node02 yum源配置 12345678910111213curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repoyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repocat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo 必备工具安装 1yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y 所有节点关闭防火墙、selinux、dnsmasq、swap。服务器配置如下： 1234567systemctl disable --now firewalld systemctl disable --now dnsmasqsystemctl disable --now NetworkManagersetenforce 0sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinuxsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config 关闭swap分区 12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 安装ntpdate 12rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpmyum install ntpdate -y 所有节点同步时间。时间同步配置如下： 123ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeecho 'Asia/Shanghai' &gt;/etc/timezonentpdate time2.aliyun.com 加入到crontab 12crontab -e*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com 所有节点配置limit： 12345678910ulimit -SHn 65535vim /etc/security/limits.conf# 末尾添加如下内容* soft nofile 655360* hard nofile 131072* soft nproc 655350* hard nproc 655350* soft memlock unlimited* hard memlock unlimited Master01节点免密钥登录其他节点： 123ssh-keygen -t rsafor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done 下载安装所有的源码文件： 12cd /root/git clone https://github.com/xukun0904/k8s-ha-install.git 所有节点升级系统并重启： 1yum update -y --exclude=kernel* &amp;&amp; reboot 4. 内核配置所有节点内核升级 123456wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-5.15.5-1.el7.elrepo.x86_64.rpmwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-5.15.5-1.el7.elrepo.x86_64.rpmyum install -y kernel-ml-5.15.5-1.el7.elrepo.x86_64.rpm kernel-ml-devel-5.15.5-1.el7.elrepo.x86_64.rpmcat /boot/grub2/grub.cfg |grep menuentrygrub2-set-default &quot;CentOS Linux (5.15.5-1.el7.elrepo.x86_64) 7 (Core)&quot;grub2-editenv list 所有节点安装ipvsadm： 1yum install ipvsadm ipset sysstat conntrack libseccomp -y 所有节点配置ipvs模块，4.19以下内核nf_conntrack_ipv4 4.19以上内核为nf_conntrack 123456789101112131415161718192021222324vim /etc/modules-load.d/ipvs.conf # 加入以下内容ip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipip 加载内核配置 1systemctl enable --now systemd-modules-load.service 开启一些k8s集群中必须的内核参数，所有节点配置k8s内核 123456789101112131415161718192021222324252627282930cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemrebootlsmod|grep -e ip_vs -e nf_conntrack 5. 基本组件安装所有节点安装Docker-ce 19.03 1yum install docker-ce-19.03.* -y 新版kubelet建议使用systemd，所以可以把cgroupdriver改为systemd 1234567cat &gt; /etc/docker/daemon.json &lt;&lt;EOF{ &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]}EOFsystemctl restart docker 所有节点设置开机自启动Docker 1systemctl daemon-reload &amp;&amp; systemctl enable --now docker Master01节点下载Kubernetes,etcd 12345# https://github.com/kubernetes/kubernetes/tree/master/CHANGELOGwget https://dl.k8s.io/v1.22.1/kubernetes-server-linux-amd64.tar.gzwget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz Master01解压Kubernetes,etcd安装文件 123tar -xf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}tar -zxvf etcd-v3.4.13-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.4.13-linux-amd64/etcd{,ctl} Maser01 Kubernetes,etcd版本查看 123kubelet --version etcdctl version 将组件发送到其他所有节点上 1234MasterNodes='k8s-master02 k8s-master03'WorkNodes='k8s-node01 k8s-node02'for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; donefor NODE in $WorkNodes; do scp /usr/local/bin/kube{let,-proxy} $NODE:/usr/local/bin/ ; done 其他节点查看版本 123kubelet --version# Master节点查看 etcdctl version 所有节点创建/opt/cni/bin目录 1mkdir -p /opt/cni/bin Master01切换分支 1234cd k8s-ha-install/ git branch -agit checkout manual-installation-v1.22.x 6.二进制生产证书 etcd证书 k8s组件证书 二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的 Master01下载生成证书工具 12345wget &quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl wget &quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson etcd证书所有Master节点创建etcd证书目录 1mkdir /etc/etcd/ssl -p 所有节点创建kubernetes相关目录 1mkdir -p /etc/kubernetes/pki Master01节点生成etcd证书（取消发送键输入到所有的会话） 生成证书的CSR文件：证书签名请求文件，配置了一些域名、公司、单位 1234# 这个目录有我们生成证书需要用到的csr文件cd /root/k8s-ha-install/pki# 生成etcd CA证书和CA证书的keycfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca 查看生成的key 12ls /etc/etcd/ssl/etcd-ca.csr etcd-ca-key.pem etcd-ca.pem 颁发证书 1234567cfssl gencert \\ -ca=/etc/etcd/ssl/etcd-ca.pem \\ -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.201,192.168.1.202,192.168.1.203 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd 查看生成的证书 12ls /etc/etcd/ssl/etcd-ca.csr etcd-ca-key.pem etcd-ca.pem etcd.csr etcd-key.pem etcd.pem 将证书复制到其他Master节点 123456789MasterNodes='k8s-master02 k8s-master03'WorkNodes='k8s-node01 k8s-node02' for NODE in $MasterNodes; do ssh $NODE &quot;mkdir -p /etc/etcd/ssl&quot; for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/${FILE} $NODE:/etc/etcd/ssl/${FILE} done done k8s组件证书Master01生成kubernetes证书 123cd /root/k8s-ha-install/pki cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca 查看生成的key 12ls /etc/kubernetes/pkica.csr ca-key.pem ca.pem 生成apiserver的客户端证书 10.96.0.是k8s service的网段，如果说需要更改k8s service网段，那就需要更改10.96.0.1，如果不是高可用集群，192.168.232.236为Master01的IP 1234cfssl gencert -ca=/etc/kubernetes/pki/ca.pem -ca-key=/etc/kubernetes/pki/ca-key.pem -config=ca-config.json -hostname=10.96.0.1,192.168.1.236,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.201,192.168.1.202,192.168.1.203 -profile=kubernetes apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserverls /etc/kubernetes/pkiapiserver.csr apiserver-key.pem apiserver.pem ca.csr ca-key.pem ca.pem 生成apiserver的聚合证书。Requestheader-client-xxx requestheader-allowwd-xxx:aggerator 123cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca cfssl gencert -ca=/etc/kubernetes/pki/front-proxy-ca.pem -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client 生成 controller-manage 的证书 123456789101112131415161718192021222324252627cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager# 注意，如果不是高可用集群，192.168.232.236:8443改为master01的地址，8443改为apiserver的端口，默认是6443# set-cluster：设置一个集群项kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.236:8443 \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig# set-credentials 设置一个用户项kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\ --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig# 设置一个环境项，一个上下文kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig# 使用某个环境当做默认环境kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig 生成 scheduler 的证书 123456789101112131415161718192021222324252627cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler # 注意，如果不是高可用集群，192.168.232.236:8443改为master01的地址，8443改为apiserver的端口，默认是6443kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.236:8443 \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=/etc/kubernetes/pki/scheduler.pem \\ --client-key=/etc/kubernetes/pki/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig 生成admin的证书 123456789101112131415cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin # 注意，如果不是高可用集群，192.168.232.236:8443改为master01的地址，8443改为apiserver的端口，默认是6443kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.1.236:8443 --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-credentials kubernetes-admin --client-certificate=/etc/kubernetes/pki/admin.pem --client-key=/etc/kubernetes/pki/admin-key.pem --embed-certs=true --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/etc/kubernetes/admin.kubeconfig kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=/etc/kubernetes/admin.kubeconfig 我们用同样的命令生成了 admin.kubeconfig，scheduler.kubeconfig，controller-manager.kubeconfig，它们之间是如何区分的？ 查看 admin-csr.json 123456789101112131415161718cat admin-csr.json { &quot;CN&quot;: &quot;admin&quot;, # 域名 &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;system:masters&quot;, # 部门，相当于admin是属于哪个组的 &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 我们生成的证书会定义一个用户 admin，它是属于 system:masters 这个组，k8s 安装的时候会有一个 clusterrole，它是一个集群角色，相当于一个配置，它有着集群最高的管理权限，同时会创建一个 clusterrolebinding，它会把 admin 绑到 system:masters 这个组上，然后这个组上的所有用户都会有这个集群的权限 创建ServiceAccount Key -&gt; secret 123openssl genrsa -out /etc/kubernetes/pki/sa.key 2048 openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub ServiceAccount 是 k8s 一种认证方式，创建 ServiceAccount 的时候会创建一个与之绑定的 secret，这个 secret 会生成一个 token 发送证书至其他节点 12345678for NODE in k8s-master02 k8s-master03; do for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do scp /etc/kubernetes/pki/${FILE} $NODE:/etc/kubernetes/pki/${FILE};done; for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do scp /etc/kubernetes/${FILE} $NODE:/etc/kubernetes/${FILE};done;done 查看证书文件（一共23个文件） 1234567[root@k8s-master01 pki]# ls /etc/kubernetes/pki/admin.csr apiserver.csr ca.csr controller-manager.csr front-proxy-ca.csr front-proxy-client.csr sa.key scheduler-key.pemadmin-key.pem apiserver-key.pem ca-key.pem controller-manager-key.pem front-proxy-ca-key.pem front-proxy-client-key.pem sa.pub scheduler.pemadmin.pem apiserver.pem ca.pem controller-manager.pem front-proxy-ca.pem front-proxy-client.pem scheduler.csr[root@k8s-master01 pki]# ls /etc/kubernetes/pki/|wc -l23 查看证书过期时间（expiry 过期时间100年） 1234567891011121314151617181920cat ca-config.json { &quot;signing&quot;: { &quot;default&quot;: { &quot;expiry&quot;: &quot;876000h&quot; }, &quot;profiles&quot;: { &quot;kubernetes&quot;: { &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;876000h&quot; } } }} 7. 高可用组件安装 Etcd配置 高可用配置 Etcd配置etcd生产环境中一定要启动奇数个节点，不然容易产生脑裂 etcd配置大致相同，注意修改每个Master节点的etcd配置的主机名和IP地址 注意三个节点的配置是不同的 Master01 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /etc/etcd/etcd.config.yml name: 'k8s-master01'data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: 'https://192.168.1.201:2380'listen-client-urls: 'https://192.168.1.201:2379,http://127.0.0.1:2379'max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: 'https://192.168.1.201:2380'advertise-client-urls: 'https://192.168.1.201:2379'discovery:discovery-fallback: 'proxy'discovery-proxy:discovery-srv:initial-cluster: 'k8s-master01=https://192.168.1.201:2380,k8s-master02=https://192.168.1.202:2380,k8s-master03=https://192.168.1.203:2380'initial-cluster-token: 'etcd-k8s-cluster'initial-cluster-state: 'new'strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: 'off'proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truepeer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: false Master02 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /etc/etcd/etcd.config.yml name: 'k8s-master02'data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: 'https://192.168.1.202:2380'listen-client-urls: 'https://192.168.1.202:2379,http://127.0.0.1:2379'max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: 'https://192.168.1.202:2380'advertise-client-urls: 'https://192.168.1.202:2379'discovery:discovery-fallback: 'proxy'discovery-proxy:discovery-srv:initial-cluster: 'k8s-master01=https://192.168.1.201:2380,k8s-master02=https://192.168.1.202:2380,k8s-master03=https://192.168.1.203:2380'initial-cluster-token: 'etcd-k8s-cluster'initial-cluster-state: 'new'strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: 'off'proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truepeer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: false Master03 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /etc/etcd/etcd.config.yml name: 'k8s-master03'data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: 'https://192.168.1.203:2380'listen-client-urls: 'https://192.168.1.203:2379,http://127.0.0.1:2379'max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: 'https://192.168.1.203:2380'advertise-client-urls: 'https://192.168.1.203:2379'discovery:discovery-fallback: 'proxy'discovery-proxy:discovery-srv:initial-cluster: 'k8s-master01=https://192.168.1.201:2380,k8s-master02=https://192.168.1.202:2380,k8s-master03=https://192.168.1.203:2380'initial-cluster-token: 'etcd-k8s-cluster'initial-cluster-state: 'new'strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: 'off'proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truepeer-transport-security: cert-file: '/etc/kubernetes/pki/etcd/etcd.pem' key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem' peer-client-cert-auth: true trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem' auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: false 所有Master节点创建etcd service并启动 12345678910111213141516vim /usr/lib/systemd/system/etcd.service [Unit]Description=Etcd ServiceDocumentation=https://coreos.com/etcd/docs/latest/After=network.target[Service]Type=notifyExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.ymlRestart=on-failureRestartSec=10LimitNOFILE=65536 [Install]WantedBy=multi-user.targetAlias=etcd3.service 所有Master节点创建etcd的证书目录 1234mkdir /etc/kubernetes/pki/etcdln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/systemctl daemon-reloadsystemctl enable --now etcd 查看etcd状态 1234567891011export ETCDCTL_API=3etcdctl --endpoints=&quot;192.168.1.203:2379,192.168.1.202:2379,192.168.1.201:2379&quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint status --write-out=table+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+| 192.168.1.203:2379 | 9eb1f144b60ad8c6 | 3.4.13 | 20 kB | false | false | 3207 | 8 | 8 | || 192.168.1.202:2379 | ef155f75af77a2e6 | 3.4.13 | 20 kB | true | false | 3207 | 8 | 8 | || 192.168.1.201:2379 | 7f890ac3cab00125 | 3.4.13 | 20 kB | false | false | 3207 | 8 | 8 | |+--------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 高可用配置注意：如果不是高可用集群或者在云上安装，haproxy和keepalived无需安装 公有云要用公有云自带的负载均衡，比如阿里云的SLB，腾讯云的ELB，用来替代haproxy和keepalived，因为公有云大部分都是不支持keepalived的，另外如果用阿里云的话，kubectl控制端不能放在master节点，推荐使用腾讯云，因为阿里云的slb有回环的问题，也就是slb代理的服务器不能反向访问SLB，但是腾讯云修复了这个问题。 所有Master节点通过yum安装HAProxy和KeepAlived： 1yum install keepalived haproxy -y 所有Master节点配置HAProxy（详细配置参考HAProxy文档，所有Master节点的HAProxy配置相同）： 1234567891011121314151617181920212223242526272829303132333435vim /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend k8s-master bind 0.0.0.0:8443 bind 127.0.0.1:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01 192.168.1.201:6443 check server k8s-master02 192.168.1.202:6443 check server k8s-master03 192.168.1.203:6443 check 所有Master节点配置KeepAlived，配置不一样，注意区分注意每个节点的IP和网卡（interface参数）Master01节点的配置： 12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { router_id LVS_DEVEL}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1}vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.1.201 virtual_router_id 51 priority 101 nopreempt advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver } } Master02节点的配置： 1234567891011121314151617181920212223242526272829303132vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { router_id LVS_DEVEL}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1 }vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.1.202 virtual_router_id 51 priority 100 nopreempt advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver } } Master03节点的配置： 12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { router_id LVS_DEVEL}vrrp_script chk_apiserver { script &quot;/etc/keepalived/check_apiserver.sh&quot; interval 5 weight -5 fall 2 rise 1}vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.1.203 virtual_router_id 51 priority 100 nopreempt advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.1.236 } track_script { chk_apiserver } } 为所有master节点配置KeepAlived健康检查文件： 1234567891011121314151617181920212223242526272829303132333435[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh #!/bin/basherr=0for k in $(seq 1 3)do check_code=$(pgrep haproxy) if [[ $check_code == &quot;&quot; ]]; then err=$(expr $err + 1) sleep 1 continue else err=0 break fidoneif [[ $err != &quot;0&quot; ]]; then echo &quot;systemctl stop keepalived&quot; /usr/bin/systemctl stop keepalived exit 1else exit 0fiscp /etc/keepalived/check_apiserver.sh k8s-master02:/etc/keepalived/scp /etc/keepalived/check_apiserver.sh k8s-master03:/etc/keepalived/chmod +x /etc/keepalived/check_apiserver.sh#启动haproxy和keepalived[root@k8s-master01 keepalived]# systemctl daemon-reload[root@k8s-master01 keepalived]# systemctl enable --now haproxy[root@k8s-master01 keepalived]# systemctl enable --now keepalived 测试VIP 12345678910111213[root@k8s-master01 ~]# ping 192.168.1.236 -c 4PING 192.168.1.236 (192.168.1.236) 56(84) bytes of data.64 bytes from 192.168.1.236: icmp_seq=1 ttl=64 time=0.464 ms64 bytes from 192.168.1.236: icmp_seq=2 ttl=64 time=0.063 ms64 bytes from 192.168.1.236: icmp_seq=3 ttl=64 time=0.062 ms64 bytes from 192.168.1.236: icmp_seq=4 ttl=64 time=0.063 mstelnet 192.168.1.236 8443Trying 192.168.1.236...Connected to 192.168.1.236.Escape character is '^]'.Connection closed by foreign host. 8.二进制K8s组件配置 Apiserver ControllerManager Scheduler 所有节点创建相关目录 1mkdir -p /etc/kubernetes/manifests/ /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes Apiserver所有Master节点创建kube-apiserver service Master01配置（取消发送键输入到所有的会话） 注意k8s service网段为10.96.0.0/12，该网段不能和宿主机的网段、Pod网段的重复，请按需修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --insecure-port=0 \\ --advertise-address=192.168.1.201 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.1.201:2379,https://192.168.1.202:2379,https://192.168.1.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failureRestartSec=10sLimitNOFILE=65535 [Install]WantedBy=multi-user.target Master02配置 注意k8s service网段为10.96.0.0/12，该网段不能和宿主机的网段、Pod网段的重复，请按需修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --insecure-port=0 \\ --advertise-address=192.168.1.202 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.1.201:2379,https://192.168.1.202:2379,https://192.168.1.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failureRestartSec=10sLimitNOFILE=65535 [Install]WantedBy=multi-user.target Master03配置 注意k8s service网段为10.96.0.0/12，该网段不能和宿主机的网段、Pod网段的重复，请按需修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --insecure-port=0 \\ --advertise-address=192.168.1.203 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.1.201:2379,https://192.168.1.202:2379,https://192.168.1.203:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\ --service-account-issuer=https://kubernetes.default.svc.cluster.local \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User # --token-auth-file=/etc/kubernetes/token.csv Restart=on-failureRestartSec=10sLimitNOFILE=65535 [Install]WantedBy=multi-user.target 所有Master节点开启kube-apiserver 1systemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserver 检测kube-server状态 1systemctl status kube-apiserver ControllerManager所有Master节点配置kube-controller-manager service 注意k8s Pod网段为172.16.0.0/12，该网段不能和宿主机的网段、k8s Service网段的重复，请按需修改 1234567891011121314151617181920212223242526272829303132vim /usr/lib/systemd/system/kube-controller-manager.service [Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-controller-manager \\ --v=2 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=172.16.0.0/12 \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --node-cidr-mask-size=24 Restart=alwaysRestartSec=10s [Install]WantedBy=multi-user.target 所有Master节点启动kube-controller-manager 123systemctl daemon-reload systemctl enable --now kube-controller-manager 查看启动状态 1systemctl status kube-controller-manager Scheduler所有Master节点配置kube-scheduler service 12345678910111213141516171819vim /usr/lib/systemd/system/kube-scheduler.service [Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-scheduler \\ --v=2 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --leader-elect=true \\ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig Restart=alwaysRestartSec=10s [Install]WantedBy=multi-user.target 启动 123systemctl daemon-reloadsystemctl enable --now kube-scheduler 查看启动状态 1systemctl status kube-scheduler 9.二进制使用Bootstrapping自动颁发证书它可以给 node 节点自动颁发证书，也就是给 keepalived 颁发证书 为什么这个证书不是手动管理？因为 k8s 主节点可能是固定的，创建好之后一直就是那几台，但是 node 节点可能变化比较多，如果添加，删除，故障维护节点的时候手动添加会比较麻烦，keepalived 证书和主机名是有绑定的，而我们的主机名又是不一样的，所以需要有一种机制自动颁发 keepalived 发来的证书请求 在Master01创建bootstrap注意，如果不是高可用集群，192.168.1.236:8443改为master01的地址，8443改为apiserver的端口，默认是6443 12345cd /root/k8s-ha-install/bootstrapkubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.1.236:8443 --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfigkubectl config set-credentials tls-bootstrap-token-user --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfigkubectl config set-context tls-bootstrap-token-user@kubernetes --cluster=kubernetes --user=tls-bootstrap-token-user --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfigkubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig bootstrap-kubelet.kubeconfig 是一个 keepalived 用来向 apiserver 申请证书的文件 注意：如果要修改bootstrap.secret.yaml的token-id和token-secret，需要保证 c8ad9c 字符串一致的，并且位数是一样的。还要保证上个命令的黄色字体：c8ad9c.2e4d610cf3e7426e与你修改的字符串要一致 123456789101112131415cat bootstrap.secret.yamlapiVersion: v1kind: Secretmetadata: name: bootstrap-token-c8ad9c namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData: description: &quot;The default bootstrap token generated by 'kubelet '.&quot; token-id: c8ad9c token-secret: 2e4d610cf3e7426e usage-bootstrap-authentication: &quot;true&quot; usage-bootstrap-signing: &quot;true&quot; auth-extra-groups: system:bootstrappers:default-node-token,system:bootstrappers:worker,system:bootstrappers:ingress 创建配置文件，缺乏此文件无法执行 kubectl get node（The connection to the server localhost:8080 was refused），需要将证书复制过来 1mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config kubectl 命令只需要一个节点拥有就可以，这是控制节点，不可以让每个节点都拥有，这样非常危险，可以把他放到集群之外的任何一个节点都可以，并不一定是我们的 k8s 节点，任何一台服务器与 k8s 相通即可，需要把这个文件复制过去，就可以访问到我们这个集群 创建 bootstrap 1kubectl create -f bootstrap.secret.yaml 10.二进制Node节点及Calico配置Node节点 复制证书 Kubelet配置 kube-proxy配置 复制证书 node节点使用自动颁发证书的形式配置 Master01节点复制证书至Node节点 1234567891011cd /etc/kubernetes/ for NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do ssh $NODE mkdir -p /etc/kubernetes/pki /etc/etcd/ssl /etc/etcd/ssl for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do scp /etc/etcd/ssl/$FILE $NODE:/etc/etcd/ssl/ done for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/${FILE} done done Kubelet配置 所有节点创建相关目录 1mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/ 所有节点配置kubelet service 123456789101112131415vim /usr/lib/systemd/system/kubelet.service# 添加以下内容[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/kubernetes/kubernetesAfter=docker.serviceRequires=docker.service[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 所有节点配置kubelet service的配置文件 123456789vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf# 添加以下内容[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2&quot;Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' &quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 所有节点创建kubelet的配置文件 注意：如果更改了k8s的service网段，需要更改kubelet-conf.yml 的clusterDNS:配置，改成k8s Service网段的第十个地址，比如10.96.0.10 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273vim /etc/kubernetes/kubelet-conf.yml# 添加以下内容apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: systemdcgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 启动所有节点kubelet 12systemctl daemon-reloadsystemctl enable --now kubelet 查看系统日志 123456789101112tail -f /var/log/messages# 显示只有如下信息为正常，因为Calico还没安装Unable to update cni config&quot; err=&quot;no networks found in /etc/cni/net.d[root@k8s-master01 k8s-ha-install]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady &lt;none&gt; 12h v1.22.1k8s-master02 NotReady &lt;none&gt; 12h v1.22.1k8s-master03 NotReady &lt;none&gt; 12h v1.22.1k8s-node01 NotReady &lt;none&gt; 12h v1.22.1k8s-node02 NotReady &lt;none&gt; 12h v1.22.1 kube-proxy注意，如果不是高可用集群，192.168.1.236:8443改为master01的地址，8443改为apiserver的端口，默认是6443 在Master01执行 12345678910111213141516cd /root/k8s-ha-installgit checkout -f manual-installation-v1.21.xkubectl -n kube-system create serviceaccount kube-proxykubectl create clusterrolebinding system:kube-proxy --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxySECRET=$(kubectl -n kube-system get sa/kube-proxy \\ --output=jsonpath='{.secrets[0].name}')JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \\--output=jsonpath='{.data.token}' | base64 -d)PKI_DIR=/etc/kubernetes/pkiK8S_DIR=/etc/kuberneteskubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.pem --embed-certs=true --server=https://192.168.1.236:8443 --kubeconfig=${K8S_DIR}/kube-proxy.kubeconfigkubectl config set-credentials kubernetes --token=${JWT_TOKEN} --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfigkubectl config set-context kubernetes --cluster=kubernetes --user=kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfigkubectl config use-context kubernetes --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig 在master01将kube-proxy的systemd Service文件发送到其他节点 如果更改了集群Pod的网段，需要更改kube-proxy/kube-proxy.conf的clusterCIDR: 172.16.0.0/12参数为pod的网段。 1234567891011for NODE in k8s-master01 k8s-master02 k8s-master03; do scp ${K8S_DIR}/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig scp kube-proxy/kube-proxy.conf $NODE:/etc/kubernetes/kube-proxy.conf scp kube-proxy/kube-proxy.service $NODE:/usr/lib/systemd/system/kube-proxy.service done for NODE in k8s-node01 k8s-node02; do scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig scp kube-proxy/kube-proxy.conf $NODE:/etc/kubernetes/kube-proxy.conf scp kube-proxy/kube-proxy.service $NODE:/usr/lib/systemd/system/kube-proxy.service done 所有节点启动kube-proxy 12345systemctl daemon-reload systemctl enable --now kube-proxysystemctl status kube-proxy 11. Calico安装以下步骤只在master01执行 1cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.21.x &amp;&amp; cd calico/ 修改calico-etcd.yaml的以下位置 12345678910111213141516# 修改calico-etcd.yaml的以下位置sed -i 's#etcd_endpoints: &quot;http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;#etcd_endpoints: &quot;https://192.168.1.201:2379,https://192.168.1.202:2379,https://192.168.1.203:2379&quot;#g' calico-etcd.yaml ETCD_CA=`cat /etc/kubernetes/pki/etcd/etcd-ca.pem | base64 | tr -d '\\n'`ETCD_CERT=`cat /etc/kubernetes/pki/etcd/etcd.pem | base64 | tr -d '\\n'`ETCD_KEY=`cat /etc/kubernetes/pki/etcd/etcd-key.pem | base64 | tr -d '\\n'` sed -i &quot;s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcd-cert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g&quot; calico-etcd.yaml sed -i 's#etcd_ca: &quot;&quot;#etcd_ca: &quot;/calico-secrets/etcd-ca&quot;#g; s#etcd_cert: &quot;&quot;#etcd_cert: &quot;/calico-secrets/etcd-cert&quot;#g; s#etcd_key: &quot;&quot; #etcd_key: &quot;/calico-secrets/etcd-key&quot; #g' calico-etcd.yaml # 更改此处为自己的pod网段POD_SUBNET=&quot;172.16.0.0/12&quot; # 注意下面的这个步骤是把calico-etcd.yaml文件里面的CALICO_IPV4POOL_CIDR下的网段改成自己的Pod网段，也就是把192.168.x.x/16改成自己的集群网段，并打开注释：sed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@# value: &quot;192.168.0.0/16&quot;@ value: '&quot;${POD_SUBNET}&quot;'@g' calico-etcd.yaml 创建calico 12345kubectl apply -f calico-etcd.yamlkubectl get nodekubectl get pods -n kube-system -o wide 12.安装CoreDNS安装对应版本（推荐） 123cd /root/k8s-ha-install/git checkout -f manual-installation-v1.22.x 如果更改了k8s service的网段需要将coredns的serviceIP改成k8s service网段的第十个IP 1sed -i &quot;s#192.168.0.10#10.96.0.10#g&quot; CoreDNS/coredns.yaml 安装coredns 123kubectl create -f CoreDNS/coredns.yamlkubectl get po -n kube-system -l k8s-app=kube-dns 安装最新版CoreDNS（不推荐） 123456789git clone https://github.com/coredns/deployment.gitcd deployment/kubernetes# ./deploy.sh -s -i 10.96.0.10 | kubectl apply -f -serviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created 查看状态 123kubectl get po -n kube-system -l k8s-app=kube-dnsNAME READY STATUS RESTARTS AGEcoredns-fb4874468-nr5nx 1/1 Running 0 49s 强制删除一直处于Terminating的pod 12345678910[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=kube-dnsNAME READY STATUS RESTARTS AGEcoredns-fb4874468-fgs2h 1/1 Terminating 0 6d20h [root@k8s-master01 ~]# kubectl delete pods coredns-fb4874468-fgs2h --grace-period=0 --force -n kube-systemwarning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.pod &quot;coredns-fb4874468-fgs2h&quot; force deleted [root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=kube-dnsNo resources found in kube-system namespace. 13. Metrics Server部署在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。 安装metrics server 12345678910111213141516cd /root/k8s-ha-install/git checkout -f manual-installation-v1.21.xcd /root/k8s-ha-install/metrics-server-0.4.x/[root@k8s-master01 metrics-server-0.4.x]# kubectl create -f .serviceaccount/metrics-server createdclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrole.rbac.authorization.k8s.io/system:metrics-server createdrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader createdclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator createdclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server createdservice/metrics-server createddeployment.apps/metrics-server createdapiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created 等待kube-system命令空间下的Pod全部启动后，查看状态 1234567891011121314151617181920kubectl get pods -n kube-system -o widekubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 192m 4% 2015Mi 53% k8s-master02 195m 4% 1622Mi 42% k8s-master03 210m 5% 1619Mi 42% k8s-node01 106m 2% 673Mi 17% k8s-node02 100m 2% 683Mi 17%kubectl top po -ANAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system calico-kube-controllers-cdd5755b9-vrkmp 4m 21Mi kube-system calico-node-6m649 29m 82Mi kube-system calico-node-925tz 35m 81Mi kube-system calico-node-m4nvq 39m 78Mi kube-system calico-node-td2gk 29m 73Mi kube-system calico-node-xmctp 26m 78Mi kube-system coredns-7684f7549-lhkgg 3m 13Mi kube-system metrics-server-64c6c494dc-4fvlj 4m 17Mi 14. Dashboard部署12345678910111213141516171819202122cd /root/k8s-ha-install/dashboard/git checkout -f manual-installation-v1.22.xkubectl create -f .serviceaccount/admin-user createdclusterrolebinding.rbac.authorization.k8s.io/admin-user creatednamespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createdWarning: spec.template.metadata.annotations[seccomp.security.alpha.kubernetes.io/pod]: deprecated since v1.19; use the &quot;seccompProfile&quot; field insteaddeployment.apps/dashboard-metrics-scraper created 安装最新版dashboard 官方GitHub地址：https://github.com/kubernetes/dashboard 可以在官方dashboard查看到最新版dashboard 1kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 创建管理员用户 12345678910111213141516171819202122vim admin.yaml# 添加以下内容apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBinding metadata: name: admin-user annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system 执行 1kubectl apply -f admin.yaml -n kube-system 在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问Dashboard的问题，参考图： 1--test-type --ignore-certificate-errors 更改dashboard的svc为NodePor,将ClusterIP更改为NodePort（如果已经为NodePort忽略此步骤)t： 12345kubectl edit svc kubernetes-dashboard -n kubernetes-dashboardkubectl get svc kubernetes-dashboard -n kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.99.166.71 &lt;none&gt; 443:31778/TCP 10m 根据自己的实例端口号，通过任意安装了kube-proxy的宿主机或者VIP的IP+端口即可访问到dashboard：访问Dashboard：https://192.168.1.236:18282（请更改18282为自己的端口），选择登录方式为令牌（即token方式） 查看token值： 1234567891011121314[root@k8s-master01 ~]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')Name: admin-user-token-dkhplNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: a01443ce-1bed-455e-915c-08a63df98d1fType: kubernetes.io/service-account-tokenData====namespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6InpXbk1wWHVCWTMwN2s5UUxXR2RobmtnRGloTXR0ZFlzT2lJU1JFVUhoMm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWRraHBsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMDE0NDNjZS0xYmVkLTQ1NWUtOTE1Yy0wOGE2M2RmOThkMWYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.R12TXkMMi3b_3zVskvhMMBMySy-U3MisgJtxMgcn8CTl6mKzXA0Ue3yuGcr-VrKCY6-sf6AFBlMWMOWdp_OzIH0vIFVpFN5nJ70eemG8Sfp7nGeYVgkQxZze6KierwCtsi_J1wLTbnur5BXv7-8iCjieLEnFNzwjZ7bNVOVx8XFDrM2o8woJYBDwhcTy8CjSNlnCJR69ved-M5avplMdDlr3J1BVi6KxORVLBobkrLWACaYMiQdtg-hVbqAT9LHEkFF4iJ-NAK8oyvKNEPCiQXNi6N0Miw1URueoUE0zpdz2F16FQonurWQUKo6-lhUuCGHgd7RDM50AKwnCJpLlRgca.crt: 1411 bytes 将token值输入到令牌后，单击登录即可访问Dashboard 15.集群验证 Pod必须能解析Service Pod必须能解析跨namespace的Service 每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53 Pod和Pod之前要能通（同namespace能通信、跨namespace能通信、跨机器能通信） 安装busybox 12345678910111213141516[root@k8s-master01 ~]# cat&lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - name: busybox image: busybox:1.28 command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent restartPolicy: AlwaysEOF 查看状态 123[root@k8s-master01 ~]# kubectl get poNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 0 29s Pod必须能解析Service 123456[root@k8s-master01 ~]# kubectl exec busybox -n default -- nslookup kubernetesServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local Pod必须能解析跨namespace的Service 123456[root@k8s-master01 ~]# kubectl exec busybox -n default -- nslookup kube-dns.kube-systemServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kube-dns.kube-systemAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local 查看kubernetes服务 12345678910kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 63mkubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 63mmetrics-server ClusterIP 10.101.237.26 &lt;none&gt; 443/TCP 22m 所有节点可以连接kubernetes和kube-dns服务 123456789telnet 10.96.0.1 443Trying 10.96.0.1...Connected to 10.96.0.1.Escape character is '^]'.telnet 10.96.0.10 53Trying 10.96.0.10...Connected to 10.96.0.10.Escape character is '^]'. 所有节点可以ping通pod的ip地址 12345678910111213141516171819202122232425[root@k8s-master01 ~]# kubectl get po --all-namespaces -owideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdefault busybox 1/1 Running 0 3m45s 172.25.92.66 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system calico-kube-controllers-cdd5755b9-vrkmp 1/1 Running 0 85m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system calico-node-6m649 1/1 Running 0 85m 192.168.1.203 k8s-master03 &lt;none&gt; &lt;none&gt;kube-system calico-node-925tz 1/1 Running 0 85m 192.168.1.202 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system calico-node-m4nvq 1/1 Running 0 85m 192.168.1.205 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system calico-node-td2gk 1/1 Running 0 85m 192.168.1.204 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system calico-node-xmctp 1/1 Running 0 85m 192.168.1.201 k8s-master01 &lt;none&gt; &lt;none&gt;kube-system coredns-7684f7549-lhkgg 1/1 Running 0 64m 172.25.92.65 k8s-master02 &lt;none&gt; &lt;none&gt;kube-system metrics-server-64c6c494dc-4fvlj 1/1 Running 0 56m 172.25.244.193 k8s-master01 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-86bb69c5f6-v57zp 1/1 Running 0 52m 172.27.14.193 k8s-node02 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-5b4489d97b-zc8tf 1/1 Running 0 52m 172.17.125.1 k8s-node01 &lt;none&gt; &lt;none&gt;ping 172.25.92.65 -c 4PING 172.25.92.65 (172.25.92.65) 56(84) bytes of data.64 bytes from 172.25.92.65: icmp_seq=1 ttl=63 time=0.365 ms64 bytes from 172.25.92.65: icmp_seq=2 ttl=63 time=0.343 ms64 bytes from 172.25.92.65: icmp_seq=3 ttl=63 time=0.559 ms64 bytes from 172.25.92.65: icmp_seq=4 ttl=63 time=0.520 ms--- 172.25.92.65 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3071msrtt min/avg/max/mdev = 0.343/0.446/0.559/0.097 ms 非当前机器的pod之间的ip互通 12345678910111213141516171819202122232425262728293031# 跨机器通信[root@k8s-master01 ~]# kubectl exec -ti calico-node-925tz -n kube-system -- bashDefaulted container &quot;calico-node&quot; out of: calico-node, install-cni (init), flexvol-driver (init)[root@k8s-master02 /]# ping 192.168.1.201 -c 4PING 192.168.1.201 (192.168.1.201) 56(84) bytes of data.64 bytes from 192.168.1.201: icmp_seq=1 ttl=64 time=0.272 ms64 bytes from 192.168.1.201: icmp_seq=2 ttl=64 time=0.307 ms64 bytes from 192.168.1.201: icmp_seq=3 ttl=64 time=0.291 ms64 bytes from 192.168.1.201: icmp_seq=4 ttl=64 time=0.247 ms--- 192.168.1.201 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 93msrtt min/avg/max/mdev = 0.247/0.279/0.307/0.025 ms[root@k8s-master02 /]# exitexit[root@k8s-master01 ~]# kubectl get po -owideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESbusybox 1/1 Running 0 12m 172.25.92.66 k8s-master02 &lt;none&gt; &lt;none&gt;[root@k8s-master01 ~]# kubectl exec -it busybox -- sh/ # ping 192.168.1.203PING 192.168.1.203 (192.168.1.203): 56 data bytes64 bytes from 192.168.1.203: seq=0 ttl=63 time=0.581 ms64 bytes from 192.168.1.203: seq=1 ttl=63 time=0.542 ms64 bytes from 192.168.1.203: seq=2 ttl=63 time=0.420 ms64 bytes from 192.168.1.203: seq=3 ttl=63 time=0.391 ms^C--- 192.168.1.203 ping statistics ---4 packets transmitted, 4 packets received, 0% packet lossround-trip min/avg/max = 0.391/0.483/0.581 ms NodePort局域网内可以正常访问，Dashboard已验证。 创建一个带有三个副本的deployment 1234567891011121314151617181920[root@k8s-master01 ~]# kubectl create deploy nginx --image=nginx --replicas=3deployment.apps/nginx created [root@k8s-master01 ~]# kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEnginx 3/3 3 3 26s # 查看部署所在的NODE[root@k8s-master01 ~]# kubectl get po -owideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESbusybox 1/1 Running 0 15m 172.25.92.66 k8s-master02 &lt;none&gt; &lt;none&gt;nginx-6799fc88d8-fv4b8 1/1 Running 0 85s 172.25.244.194 k8s-master01 &lt;none&gt; &lt;none&gt;nginx-6799fc88d8-hzjq8 1/1 Running 0 85s 172.27.14.194 k8s-node02 &lt;none&gt; &lt;none&gt;nginx-6799fc88d8-mmzbm 1/1 Running 0 85s 172.17.125.2 k8s-node01 &lt;none&gt; &lt;none&gt; # 删除[root@k8s-master01 ~]# kubectl delete deploy nginxdeployment.apps &quot;nginx&quot; deleted[root@k8s-master01 ~]# kubectl delete po busyboxpod &quot;busybox&quot; deleted 16.生产环境k8s集群关键性配置 docker配置 controller-manager配置 kubelet配置 kubelet-conf.yml docker配置所有节点 1234567891011121314151617vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot;, &quot;http://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;max-concurrent-downloads&quot;: 10, # 并发下载的线程数 &quot;max-concurrent-uploads&quot;: 5, # 并发上传的线程数 &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;300m&quot;, # 限制日志文件最大容量，超过则分割 &quot;max-file&quot;: &quot;2&quot; # 日志保存最大数量 }, &quot;live-restore&quot;: true # 更改docker配置之后需要重启docker才能生效，这个参数可以使得重启docker不影响正在运行的容器进程} 更新配置 12systemctl daemon-reloadsystemctl restart docker controller-manager配置Master节点添加证书过期时间 123vim /usr/lib/systemd/system/kube-controller-manager.service --cluster-signing-duration=876000h0m0s \\ 更新配置 123systemctl daemon-reload systemctl restart kube-controller-manager kubelet配置所有节点 123456789vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf [Service] Environment=&quot;KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.kubeconfig --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig&quot; Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot; Environment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot; Environment=&quot;KUBELET_EXTRA_ARGS=--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --image-pull-progress-deadline=30m&quot; ExecStart= ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 修改了以下内容 k8s默认加密方式会被识别为漏洞，需要修改加密方式 1--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 kubelet下载镜像的deadline，避免重复循环加载 1--image-pull-progress-deadline=30m kubelet-conf.yml123456789101112131415vim /etc/kubernetes/kubelet-conf.yml # 添加如下配置rotateServerCertificates: trueallowedUnsafeSysctls: # 允许修改内核，才能修改内核参数，比如修改最大并发量，但是涉及到安全问题，所以按需配置 - &quot;net.core*&quot; - &quot;net.ipv4.*&quot;kubeReserved: # 预留资源，生产环境需要设置高一点，预留足够资源 cpu: &quot;10m&quot; memory: 10Mi ephemeral-storage: 10MisystemReserved: cpu: &quot;10m&quot; memory: 20Mi ephemeral-storage: 1Gi 更新配置 123systemctl daemon-reload systemctl restart kubelet 验证 1234567891011systemctl status kubelettail -n 10 /var/log/messages[root@k8s-master01 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 Ready &lt;none&gt; 14h v1.22.1k8s-master02 Ready &lt;none&gt; 14h v1.22.1k8s-master03 Ready &lt;none&gt; 14h v1.22.1k8s-node01 Ready &lt;none&gt; 14h v1.22.1k8s-node02 Ready &lt;none&gt; 14h v1.22.1","link":"/2021-12-05/bea16a9fbd52/"}],"tags":[{"name":"MariaDB","slug":"MariaDB","link":"/tags/MariaDB/"},{"name":"JDK","slug":"JDK","link":"/tags/JDK/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"MyBatis-Plus","slug":"MyBatis-Plus","link":"/tags/MyBatis-Plus/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"}],"categories":[{"name":"工具安装","slug":"工具安装","link":"/categories/%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"后端","slug":"工具安装/后端","link":"/categories/%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/%E5%90%8E%E7%AB%AF/"},{"name":"云原生","slug":"学习笔记/云原生","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"前端","slug":"工具安装/前端","link":"/categories/%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/%E5%89%8D%E7%AB%AF/"},{"name":"日常随笔","slug":"日常随笔","link":"/categories/%E6%97%A5%E5%B8%B8%E9%9A%8F%E7%AC%94/"},{"name":"问题记录","slug":"日常随笔/问题记录","link":"/categories/%E6%97%A5%E5%B8%B8%E9%9A%8F%E7%AC%94/%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"}]}